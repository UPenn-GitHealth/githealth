# -*- coding: utf-8 -*-
"""10_17_2023_work_v5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rDSC039ANw_IFcJX8LSqLVHdVPmp6VlI
"""

import requests
import csv
import json
import pandas as pd
import numpy as np

# Replace with your GitHub personal access token
access_token = "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"

# GitHub API endpoint
base_url = "https://api.github.com"

# Repository details
owner = "autowarefoundation"
repo_name = "autoware"

# Define the year you want to filter discussions for
year = "2022"

# Initialize data containers
discussions_data = []

# Prepare headers with authorization
headers = {
    "Authorization": f"Bearer {access_token}"
}

# Define the API endpoint for issues in the repository
issues_endpoint = f"/repos/{owner}/{repo_name}/issues"

# Function to fetch comments for an issue
def fetch_comments(issue_number):
    comments_url = f"{base_url}{issues_endpoint}/{issue_number}/comments"
    response = requests.get(comments_url, headers=headers)
    if response.status_code == 200:
        return response.json()
    return []

# Fetch the comments for issue 3
issue_comments = fetch_comments(3)

# Use json.dumps with indent to make the JSON response readable
print(json.dumps(issue_comments, indent=4))

# Fetch all issues (discussions)
page = 1
while True:
    params = {
        "state": "all",
        "per_page": 100,
        "page": page
    }
    response = requests.get(base_url + issues_endpoint, headers=headers, params=params)
    if response.status_code == 200:
        issues = response.json()
        if not issues:
            break
        for issue in issues:
            # Check if the issue was created in the specified year
            created_at = issue["created_at"][:4]  # Extract the year part
            if created_at == year:
                discussion_data = {
                    "title": issue["title"],
                    "user_id": issue["user"]["login"],
                    "created_at": issue["created_at"],
                    "comments": []
                }
                # Fetch comments for this issue
                comments = fetch_comments(issue["number"])
                for comment in comments:
                    comment_data = {
                        "user_id": comment["user"]["login"],
                        "created_at": comment["created_at"],
                        "comment_text": comment["body"]
                    }
                    discussion_data["comments"].append(comment_data)
                discussions_data.append(discussion_data)
        page += 1
    else:
        print(f"Failed to fetch issues. Status code: {response.status_code}")
        break

# Create a CSV file with the discussions data
csv_file_path = f"discussions_{year}.csv"
with open(csv_file_path, "w", newline="", encoding="utf-8") as csv_file:
    fieldnames = ["title", "user_id", "created_at", "comments"]
    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
    writer.writeheader()
    for discussion in discussions_data:
        writer.writerow(discussion)

# Create a JSON file with the discussions data
json_file_path = f"discussions_{year}.json"
with open(json_file_path, "w", encoding="utf-8") as json_file:
    json.dump(discussions_data, json_file, indent=4)

print(f"Data for year {year} has been saved to {csv_file_path} and {json_file_path}")

discussion_df = pd.read_csv(f"discussions_{year}.csv")

discussion_df.head(5)

# Read the JSON file
with open(f"discussions_{year}.json", "r", encoding="utf-8") as json_file:
    data = json.load(json_file)

# Print the first 1-2 entries in a pretty JSON format
print(json.dumps(data[:2], indent=4, ensure_ascii=False))

# Create a new data structure with the desired format
new_data = []

for discussion in discussions_data:
    thread_id = len(new_data) + 1  # Assign unique thread ID
    thread_creator = discussion["user_id"]  # Get the thread creator's username

    # Create a placeholder for threads without comments
    if not discussion["comments"]:
        comment_data = {
            "title": discussion["title"],  # Include the title
            "Created_at": discussion["created_at"],
            "thread_id": thread_id,
            "subtitle_id": "",  # Empty for threads without comments
            "Type": "",  # Empty for threads without comments
            "Comment_text": "",  # Empty for threads without comments
            "creator": thread_creator  # Include the thread creator
        }
        new_data.append(comment_data)

    for index, comment in enumerate(discussion["comments"], start=1):
        comment_creator = comment["user_id"]
        if comment_creator == thread_creator:
            comment_type = "questioned"
        else:
            comment_type = "answered"

        comment_data = {
            "title": discussion["title"],  # Include the title
            "Created_at": comment["created_at"],
            "thread_id": thread_id,
            "subtitle_id": index,  # Assign unique subtitle ID
            "Type": comment_type,
            "Comment_text": comment["comment_text"],
            "creator": comment_creator  # Include the comment creator
        }
        new_data.append(comment_data)

# Create a new CSV file with the desired columns
csv_file_path = f"discussions_{year}_new.csv"
with open(csv_file_path, "w", newline="", encoding="utf-8") as csv_file:
    fieldnames = ["title", "Created_at", "thread_id", "subtitle_id", "Type", "Comment_text", "creator"]
    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
    writer.writeheader()
    for data in new_data:
        writer.writerow(data)

print(f"Data analysis and manipulation completed. New data saved to {csv_file_path}")

discussion_new_df = pd.read_csv(f"discussions_{year}_new.csv")

discussion_new_df.head(5)