# -*- coding: utf-8 -*-
"""PR_11_28_2023_Submission_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_2H5Gl7sciYkfK4RqXFeXjglp5FBk6Q-

Create pull requests database
"""

import requests
import csv
import pandas as pd
from datetime import datetime, timedelta

# Replace with your GitHub personal access token
access_token = "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"

# GitHub API endpoint
base_url = "https://api.github.com"

# Repository details
owner = "autowarefoundation"
repo_name = "autoware"

# Initialize data containers
pull_requests_data = []

# Prepare headers with authorization
headers = {
    "Authorization": f"Bearer {access_token}"
}

# Function to fetch details for a pull request
def fetch_pull_request_details(pull_request_number):
    pr_details_url = f"{base_url}/repos/{owner}/{repo_name}/pulls/{pull_request_number}"
    response = requests.get(pr_details_url, headers=headers)
    if response.status_code == 200:
        return response.json()
    return None

# Function to fetch check runs for a pull request
def fetch_check_runs(pull_request_number):
    pr_details = fetch_pull_request_details(pull_request_number)

    if pr_details is not None and "head" in pr_details:
        commit_sha = pr_details["head"]["sha"]
        check_runs_url = f"{base_url}/repos/{owner}/{repo_name}/commits/{commit_sha}/check-runs"
        response = requests.get(check_runs_url, headers=headers)

        if response.status_code == 200:
            check_runs_data = response.json()

            # Count the number of check runs for the commit
            return check_runs_data["total_count"]

    return 0

# Function to fetch pull request review comments
def fetch_review_comments(pull_request_number):
    review_comments_url = f"{base_url}/repos/{owner}/{repo_name}/pulls/{pull_request_number}/comments"
    response = requests.get(review_comments_url, headers=headers)

    if response.status_code == 200:
        return response.json()
    return []

# Function to fetch pull requests for a specific year
def fetch_pull_requests_by_year(year):
    page = 1
    while True:
        params = {
            "state": "all",
            "per_page": 100,
            "page": page
        }
        response = requests.get(f"{base_url}/repos/{owner}/{repo_name}/pulls", headers=headers, params=params)

        # Print rate limit information
        rate_limit_url = f"{base_url}/rate_limit"
        rate_limit_response = requests.get(rate_limit_url, headers=headers)
        rate_limit_info = rate_limit_response.json()
        print(f"Rate Limit - Limit: {rate_limit_info['resources']['core']['limit']}, Remaining: {rate_limit_info['resources']['core']['remaining']}")

        if response.status_code == 200:
            pull_requests = response.json()
            if not pull_requests:
                break
            for pull_request in pull_requests:
                # Convert ISO 8601 date to a datetime object
                pr_created_at = datetime.strptime(pull_request["created_at"], "%Y-%m-%dT%H:%M:%SZ")

                # Check if the pull request was created in the specified year
                if pr_created_at.year == year:
                    pull_request_details = fetch_pull_request_details(pull_request["number"])
                    if pull_request_details is not None and "commits" in pull_request_details:
                        pull_request_checks = fetch_check_runs(pull_request["number"])

                        pull_request_data = {
                            "pull_request_id": pull_request["id"],
                            "pull_request_number": pull_request["number"],
                            "pull_request_user_id": pull_request["user"]["login"],
                            "pull_request_created_at": pull_request["created_at"],
                            "pull_request_title": pull_request["title"],
                            "pull_request_commits": pull_request_details["commits"],
                            "pull_request_checks": pull_request_checks,
                            "pull_request_files_changed": pull_request_details["changed_files"],
                            "pull_request_lines_changed": pull_request_details["additions"] + pull_request_details["deletions"],
                        }
                        pull_requests_data.append(pull_request_data)

            page += 1
        elif response.status_code == 403:
            print(f"Forbidden. Rate limit exceeded or authentication issue.")
            reset_time = datetime.fromtimestamp(rate_limit_info['resources']['core']['reset'])
            current_time = datetime.utcnow()
            time_until_reset = reset_time - current_time
            print(f"Time until reset: {time_until_reset}")
            # Delay the execution until the reset time
            delay_seconds = (time_until_reset.total_seconds() + 1)  # add 1 second to be safe
            print(f"Waiting for {delay_seconds} seconds before retrying...")
            time.sleep(delay_seconds)
        else:
            print(f"Failed to fetch pull requests. Status code: {response.status_code}")
            break

# Specify the year you want to fetch data for
target_year = 2023

# Fetch pull requests for the specified year
fetch_pull_requests_by_year(target_year)

# Create a list to store the final data
final_data = []

# Iterate over pull requests and fetch comments
for pull_request in pull_requests_data:
    comments = fetch_review_comments(pull_request["pull_request_number"])

    # Initialize the row with fields not related to comments
    row = {
        "pull_request_id": pull_request["pull_request_id"],
        "pull_request_number": pull_request["pull_request_number"],
        "pull_request_user_id": pull_request["pull_request_user_id"],
        "pull_request_created_at": pull_request["pull_request_created_at"],
        "pull_request_title": pull_request["pull_request_title"],
        "pull_request_commits": pull_request["pull_request_commits"],
        "pull_request_checks": pull_request["pull_request_checks"],
        "pull_request_files_changed": pull_request["pull_request_files_changed"],
        "pull_request_lines_changed": pull_request["pull_request_lines_changed"],
        "comment_id": None,
        "comment_created_at": None,
        "comment_text": None,
        "comment_author": None,
        "comment_parent_id": None,
    }

    if comments is not None:
        # Duplicate pull request information for each comment
        for comment in comments:
            if comment is not None:
                comment_id = comment.get("id", None)
                comment_parent_id = comment.get("in_reply_to_id", None)
                comment_user_id = comment.get("user", {}).get("login") if comment.get("user") is not None else None

                # Update the fields related to comments
                row.update({
                    "comment_id": comment_id,
                    "comment_created_at": comment.get("created_at", None),
                    "comment_text": comment.get("body", None),
                    "comment_author": comment_user_id,
                    "comment_parent_id": comment_parent_id,
                })

                # Append the row to the final data
                final_data.append(row.copy())  # Use copy to avoid overwriting the same dictionary reference
    else:
        # If there are no comments, append the row with None values for comment-related fields
        final_data.append(row)

# Create a DataFrame with the final data
pull_requests_comments_df = pd.DataFrame(final_data)

# Convert 'comment_parent_id' to integers if the column exists
if 'comment_parent_id' in pull_requests_comments_df.columns:
    pull_requests_comments_df['comment_parent_id'] = pull_requests_comments_df['comment_parent_id'].astype('Int64')

# Display the first 10 rows of the DataFrame
print(pull_requests_comments_df.head(10))

# Display information about the DataFrame
print(pull_requests_comments_df.info())

# Save the DataFrame to a CSV file
csv_file_path = f"pull_requests_data_{target_year}.csv"
pull_requests_comments_df.to_csv(csv_file_path, index=False)

print(f"Data analysis and manipulation completed. Data saved to {csv_file_path}")